# Pipeline for additional processing of ASV FASTA files through HoneyPis
#
# Example call: snakemake -s workflow/Snakefile --configfile config/config.yaml --use-conda --conda-prefix ${CONDA_PREFIX}/pipeline --cores 1 -rpn
# Example call with conda-prefix-path: snakemake -s workflow/Snakefile --configfile config/config.yaml --use-conda --conda-prefix ${CONDA_PREFIX}/pipeline --cores $CORES -rpn

##############################
# MODULES
import os, re
import glob
import pandas as pd


##############################
# CONFIG
# can be overwritten by using --configfile <path to config> when calling snakemake
# configfile: "config/config.yaml"


##############################
# Paths
SRC_DIR = srcdir("../scripts")
ENV_DIR = srcdir("../envs")


##############################
# default executable for snakemake
# shell.executable("bash")


##############################
# Relevant directories
DATA_DIR = config["data_dir"]
FASTA = config["fasta"]
RESULTS_DIR = config["results_dir"]
CONFIDENCE = config["confidence"]
AMPLICON_TYPE = config["amplicon_type"]

##############################
# Run
rule all:
    input:
        expand(os.path.join(RESULTS_DIR, "taxonomy/assigned_taxonomy_filtered_{confidence}.txt"), confidence=CONFIDENCE),
        os.path.join(RESULTS_DIR, "blast/taxonomy_table.txt")
    output:
        touch("status/rdp_classifier.done")


# Downloading the database
rule download_db:
    output:
        expand(os.path.join(RESULTS_DIR, "db/Gweon-{amplicon}-20200325-rdp-trained/Gweon-{amplicon}.properties"), amplicon=AMPLICON_TYPE)
    log:
        os.path.join(RESULTS_DIR, "logs/download_db.log")
    message:
        "Downloading the database required for classification"
    params:
        url=config["database"][AMPLICON_TYPE]
    wildcard_constraints:
        amplicon="|".join(AMPLICON_TYPE)
    shell:
        "(date && "
        "wget -P $(dirname $(dirname {output})) {params.url} &&"
        "cd $(dirname $(dirname {output})) && tar -xzvf $(basename {params.url}) &&"
        "date) &> {log}"

# Running the classifier
rule rdp_classifier:
    input:
        fa=os.path.join(FASTA),
        db=rules.download_db.output[0]
    output:
        os.path.join(RESULTS_DIR, "taxonomy/assigned_taxonomy_prelim.txt")
    log:
        os.path.join(RESULTS_DIR, "logs/rdp_classifier_prelim.log")
    conda:
        os.path.join(ENV_DIR, "rdp_classifier.yaml")
    params:
        mem=config["mem"]
    message:
        "Running the RDP classifier on the ASV input"
    shell:
        "(date && "
        "rdp_classifier -Xmx{params.mem} classify -t {input.db} -o {output} {input.fa} && "
        "date) &> {log}"

# Reformat the taxonomy
rule reformat:
    input:
        rules.rdp_classifier.output[0]
    output:
        os.path.join(RESULTS_DIR, "taxonomy/assigned_taxonomy_filtered_{confidence}.txt")
    log:
        os.path.join(RESULTS_DIR, "logs/rdp_classifier_{confidence}.log")
    params:
        SRC=os.path.join(SRC_DIR, "honeypi_reformatAssignedTaxonomy.py")
    message:
        "Applying the confidence score of {wildcards.confidence} to the taxonomy output"
    shell:
        "(date && "
        "python3 {params.SRC} -i {input} -o {output} -c {wildcards.confidence} && "
        "date) &> {log}"

# Running BLAST
rule run_qblast_nt:
    input:
        query_fasta=os.path.join(FASTA)
    output:
        table=os.path.join(RESULTS_DIR, "blast/taxonomy_table.txt")
    log:
        os.path.join(RESULTS_DIR, "logs/blast_nt.log")
    params:
        blast_program=config["blast"]["program"],
        blast_database=config["blast"]["db"],
        qblast=os.path.join(SRC_DIR, "run_qblast.py")
    conda:
        os.path.join(ENV_DIR, "biopython.yaml")
    message:
        "Running BLAST on ASV FASTA input"
    shell:
        "(date && "
        "{params.qblast} {input.query_fasta} {params.blast_program} {params.blast_database} {output.table} &&"
        "date) &> {log}"

#        # importing functions for BLAST + Taxonomy
#        from scripts.utils import run_qblast, get_taxonomy_from_taxid
#        
#        # Running BLAST
#        query_sequence = SeqIO.read(input.query_fasta, "fasta").seq
#        table = run_qblast(query_sequence, params.blast_program, params.blast_database)
#        with open(output.table, "w") as table_file:
#            table_file.write("Query_ID\tTaxID\tTaxonomy\n")
#            for row in table:
#                table_file.write("\t".join(row) + "\n")
